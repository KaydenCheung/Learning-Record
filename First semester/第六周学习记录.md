#### 第六周学习记录



##### 10.21

🏆语言模型

基于统计：n-gram

缺点：1 不能依赖长上下文（因为只参考前n位，所以也许前面有对后面产生影响的词语，却因为长度而没有考虑进去）。

​           2 泛化能力差。（因为它以词频统计的形式，所以词语出现的越多，其概率就会越高，否则概率就低）。

基于神经网络：如word2vec

缺点：词通常有多义，如果将不同的语义的词编码成单个向量，那么是存在一定问题的。





##### 10.22

🏆 基于attention的seq2seq机器翻译的代码实现

🏆 理解transformer： ✈ self-attention和attention的区别

​                                        ✈ self-attention不考虑次序

​                                        ✈ 位置编码

 									   ✈ Layer Normalization

​                                        ✈ 残差连接（粗步了解）

🏆 下游任务：利用预训练模型或组件的监督学习任务

🏆 ELMO： ELMO由多层LSTM实现，**浅层往往蕴含的是句法，语法信息，而高层蕴含的是语义信息**，可以根据需要选择不同层的输出





##### 10.23

🏆 推荐系统综述论文：**Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions**

![v2-6ccabbfbe183b7d54e39b6b3c73e3b6c_720w.png](https://i.loli.net/2020/10/23/Ey1YeqCpM65HZfg.png)



✈ **Content-based recommendations（基于内容的推荐）**

基于信息检索的传统启发式方法（余弦相似性度量就是一个启发式公式），除此之外，还有贝叶斯分类器、聚类、决策树以及神经网络等等。

![QQ截图20201023144048.png](https://i.loli.net/2020/10/23/zUwVilSQZCtF2Hm.png)

缺点：✈ Limited Content Analysis  

> - 为了有足够的特征集，内容必须是可以由计算机自动解析的形式（例如，文本），或者应该手动地将特征分配给物品。但是一些领域难以自动提取特征，由于资源限制手动分配特征不切实际。
> - 内容分析不合理，两个不相同的物品可能拥有同样的内容。因此，由于基于文本的文档通常由最重要的关键字表示，因此基于内容的系统无法区分写得好的文章和写得不好的文章，如果它们碰巧使用相同的术语。

​           ✈ Overspecialization

> - 难以推荐到用户完全没接触过的物品。引入随机性，来推荐完全没接触过的物品。
> - 好的推荐系统不仅需要过滤掉完全不相同的物品，也需要过滤掉基本一样的物品。



✈ **Collaborative Methods（协同过滤）** 

​	**✈ Memory-based CF（基于内存的推荐系统）**

​				✈ Item-based CF          ✈ User-based CF

​    **✈ Model-based CF（基于模型的协同过滤推荐）**

<img src="https://i.loli.net/2020/10/23/8n2JKry9SAEzsNG.png" alt="QQ截图20201023151357.png" style="zoom:80%;" />

缺点：

> - New User Problem用户冷启动
> - New Item Problem物品冷启动
> - Sparsity数据稀疏



✈ **Hybrid Recommendation（混合推荐系统）**





##### 10.24

🏆 **Improving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion** 

✈ 会话推荐系统概要

推荐问题通常被抽象成矩阵填充/重构问题，主要的思想是对用户-评分矩阵中的缺省值填充预测，随后进行协同过滤计算．这是传统协同过滤以及隐语义模型的研究思路，但是现在有许多网站是不需要用户注册的，也就是说用户是匿名出现的，系统并不能根据该匿名用户过去的兴趣记录作出推荐。利用短期会话信息能弥补这样的不足． 当用户的长期配置文件不存在时，就需要根据用户和站点交互的会话日志来做出推荐． 这种仅依赖于用户当前进行的会话中的动作序列来预测用户下一个动作的问题被称之为基于会话的推荐。

基于会话的推荐主要算法包括：✈ 使用序列模式挖掘技术来预测用户的下一个行为   ✈ 基于马尔可夫模型的方法   ✈ 基于深度学习中的循环神经网络的方法

> 参考：
>
> [论文笔记——会话推荐系统]: https://dannyzhang.tech/?p=341



🏆 **知识图谱技术综述** 

![QQ截图20201024151251.png](https://i.loli.net/2020/10/24/jUMmK2TPHI8lVAt.png)

![QQ截图20201024154614.png](https://i.loli.net/2020/10/24/XpriHh8dOJvaU7I.png)



##### 10.25 - 26

🏆 GCN

[简单实例：]: https://blog.csdn.net/sdu_hao/article/details/104143731

我们的每一层GCN的输入都是邻接矩阵A和node的特征H，那么我们直接做一个内积，再乘一个参数矩阵W，然后激活一下，就相当于一个**简单的神经网络层**，是不是也可以呢？
$$
f(H^{(l)}, A) = \sigma\left( AH^{(l)}W^{(l)}\right) \,
$$
实验证明，即使就这么简单的神经网络层，就已经很强大了。这个简单模型应该大家都能理解吧，这就是正常的神经网络操作。
但是这个简单模型有几个局限性：

- 只使用A的话，由于A的对角线上都是0，所以在和特征矩阵H相乘的时候，只会计算一个node的所有邻居的特征的加权和，该node自己的特征却被忽略了。因此，**我们可以做一个小小的改动，给A加上一个单位矩阵I**，这样就让对角线元素变成1了。
- A是没有经过归一化的矩阵，这样与特征矩阵相乘会改变特征原本的分布，产生一些不可预测的问题。所以我们对A做一个标准化处理。首先让A的每一行加起来为1，我们可以乘以一个 ![[公式]](https://www.zhihu.com/equation?tex=D%5E%7B-1%7D) ，D就是度矩阵。我们可以进一步把 ![[公式]](https://www.zhihu.com/equation?tex=D%5E%7B-1%7D) 拆开与A相乘，得到一个对称且归一化的矩阵： ![[公式]](https://www.zhihu.com/equation?tex=D%5E%7B-1%2F2%7DAD%5E%7B-1%2F2%7D) 。

通过对上面两个局限的改进，我们便得到了最终的层特征传播公式：
$$
f(H^{(l)}, A) = \sigma\left( \hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}\right) \, ,
$$
其中 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BA%7D%3DA%2BI+) , ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BD%7D) 为 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7BA%7D+) 的degree matrix。

公式中的 ![[公式]](https://www.zhihu.com/equation?tex=D%5E%7B-1%2F2%7DAD%5E%7B-1%2F2%7D) 与对称归一化拉普拉斯矩阵十分类似，而在谱图卷积的核心就是使用对称归一化拉普拉斯矩阵，这也是GCN的卷积叫法的来历。

三篇关于拉普拉斯和傅里叶变换的讲解：

https://bbs.cvmart.net/articles/281/cong-cnn-dao-gcn-de-lian-xi-yu-qu-bie-gcn-cong-ru-men-dao-jing-fang-tong-qi?order_by=created_at&

https://zhuanlan.zhihu.com/p/120311352

https://zhuanlan.zhihu.com/p/67522582





🏆 互信息

> 定义：指的是两个随机变量之间的相关程度。
>
> 理解：确定随机变量X的值后，另一个随机变量Y不确定性的削弱程度，因而互信息取值最小为0，意味着给定一个随机变量对确定一另一个随机变量没有关系，最大取值为随机变量的熵，意味着给定一个随机变量，能完全消除另一个随机变量的不确定性。这个概念和条件熵相对。
>
> 
>
> 其实这个准则做的事情很简单：**最大化系统输入和输出间的互信息**。 那这么做有什么好处呢？
>
> - 1、可以让输出包含更多关于输入的信息，而且输出会更focus在输入中较为频繁出现的模式上，即文中提到的map distortions
> - 2、可以使输出的冗余度减小
>
> 关于第二点，笔者认为可以从信息论的角度考虑：系统可看成是连接输入X和输出Z的信道，而互信息![[公式]](https://www.zhihu.com/equation?tex=I%28X%3BZ%29)表示在信道上传输信息时，平均每个符号传递的信息量，最大化![[公式]](https://www.zhihu.com/equation?tex=I%28X%3BZ%29)也就等价于用更少的符号去传递更多的信息；在embedding方面，就是用更小的嵌入空间去表达更丰富的信息，因此该嵌入空间的冗余度很小。
>

