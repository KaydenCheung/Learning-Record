

大家好，今天我要分享的是推荐领域的一篇论文，题目是针对基于会话的推荐进行持续学习的自适应蒸馏样本重放，作者来自苏黎世联邦理工学院。它发表于RecSys，2020。论文通过样本重放和加入知识蒸馏损失使得推荐系统在持续学习新数据的同时，也能记住之前所学习到的旧知识。

首先，一个会话就是用户从第一次浏览开始在一段时间内的一个点击序列。在下面的图中就展示了5个不同的会话。基于会话的推荐，就是根据用户在当前会话中已经交互的物品，来预测用户接下来可能对什么物品感兴趣。

在这之前，已经有各种基于神经网络的方法获得了成功，但它们都是使用静态数据集以离线方式开发的。但是在实际的场景中，推荐器需要不断地更新，以适应新的和过时的数据。所以我们希望模型可以进行持续的学习，但是持续学习存在灾难性的遗忘问题，也就是说模型在新数据上学习之后往往会忘记在旧数据上所学到的信息，针对这样的问题，这篇论文提出了ADER这个模型。

这张图就是持续学习的实现流程，这里 $D_t$ 指的是第 $t$ 个周期到来的新数据，$f(\theta_t)$ 指的是第 $t$ 个周期的模型，在每一个周期 $t$ ,我们用 $D_t$ 的数据来训练模型$f(\theta_t)$ ，然后用下一周期的数据对该模型进行评估，并继续用该新数据训练下一周期的模型，整个就是不断循环的过程。

这个就是论文所提出的模型，为了避免让模型忘记之前所学到的知识，论文采用了Herding方法每次在旧数据中挑选出一部分有代表性的样本重新放到模型中再次训练，这里 $E_t$ 指的就是从前 $t$ 个周期中选择出来的旧样本，同时加入知识蒸馏损失来加强巩固旧知识。这里的$f(\theta)$使用了18年所提出的SASRec模型，该模型其实就是self-attention的一个应用。

在样本选择方面，首先要考虑的问题是对于每个出现过的item（或者说是label）应该存储多少个样本，每个item存储的样本数应该与其出现频率成正比，这一项就表示了 item $i$ 在 $t$ 这个周期所出现的频率，如果我们已经规定总共存储N个样本的话，就可以计算出每个item在 $t$ 这个周期应该存储的数量。接下来要考虑的问题是以什么样的标准来选择样本，论文采用了Herding方法，首先对每个item $i$ ，根据其所有的样本计算出一个平均特征向量 $\mu$，然后选出最接近$\mu$的 $m_i$ 个样本。

为了减少内存的开销，我们存储的样本数量是比较小的，如果只使用这些数据来防止忘记用户之前的偏好，可能约束不够强，所以论文又加入了知识蒸馏损失来巩固旧知识。该损失函数只作用在旧数据上，$\hat p$是由$t-1$时刻的模型所预测得到的分布，$p$是由 $t$ 时刻的模型所计算得到的预测。因为这些输出是通过网络内部各种信息后得出来的，我们的目标是让现在模型关于样本的输出尽可能和之前模型的输出接近，从而保留旧模型学到的信息。

另一方面，对于该周期的新数据，我们用交叉熵损失来衡量模型的性能，最后，我们通过一个自适应权重$\lambda_t$将两个损失函数结合起来，$\lambda$在每个周期都是不同的。在当前周期中新出现的item数量较少或者新数据量较小时，$\lambda$ 的值就越大，说明这个时候应该更注重 ![[公式]](https://www.zhihu.com/equation?tex=L_%7BKD%7D) 。



论文的实验是在两个数据集上进行的，它们分别是两个电子商务网站5个多月和6个多月的点击流数据。为了模拟持续学习，将数据按照时间划分为17个周期，这里的new actions指的是用户在当前周期与新出现的item交互的比例，可以看出第一个数据的动态性更强，也就是出现新的item的概率更高。

采用Recall和MRR（平均倒数排名）进行评估，最后计算16个更新周期中的平均值。

论文与4个方法进行了比较：

1）Finetune：在每个周期，只用当前周期的数据去训练模型

2）Dropout：在Finetune的基础上，在self-attention和前向层加入dropout。

3）EWC：对参数增加限制，使得重要的参数更新幅度尽量小。

4）Joint：在每个周期中，使用所有历史数据和当前数据对模型进行训练。

从结果看来，论文的模型ADER效果是最好的，在动态性较低的第二个数据集上，这些方法的数据其实都差不多，因为旧的item会经常重复出现，缓解了遗忘问题。但是在更具动态性的第一个数据集上，ADER的表现更优于其他方法。

这张图展示了每个更新周期的详细数据，可以看到在动态性较低的第二个数据集上，ADER的优势主要在于刚开始阶段，因为开始时对于新的item的操作相对较多。

这是第一篇在基于会话的推荐上进行持续学习的论文，关于持续学习或者说增量学习的研究，之前不少研究者们也是采用了样本重放和加入额外的知识蒸馏损失这种比较主流的方法，所以我觉得这篇论文在方法上面也没有很大的创新性，主要还是在不同领域的一个应用。



为什么ADER好过Joint ?

我觉得对于会话推荐来说，更重要的还是当前的一些新数据，对于一些时间比较长的历史数据，更重要的是从中学习到用户的长期偏好，而论文每次选择的样本数就是根据物品出现频率来定的，感觉可以加强学习到长期偏好。

